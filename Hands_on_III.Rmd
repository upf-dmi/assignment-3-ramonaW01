---
title: "Identification of Key Proteomic Predictors of COVID-19 Severity Using Random Forest"
subtitle: "Machine learning-based feature selection and validation against published results"
author: "Ramona Maria Walch (ramonamaria.walch01@estudiant.upf.edu)"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    theme: cosmo
    highlight: tango
    code_folding: hide
    fig_caption: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 7,
  fig.height = 5
)

```

This analysis applies a Random Forest classifier to identify the most important proteomic predictors of COVID-19 severity and validates these findings against published proteomic signatures.

# Exercise 1

**Key finding:** The Random Forest achieved high AUC but low sensitivity, indicating limited ability to detect Severe cases.


**Build a Random Forest model to classify COVID-19 severity**

Using the training cohort provided in Supplementary Table 3, build a Random Forest classification model similar to the one in the publication to distinguish between non-Severe and Severe COVID-19 patients.

Report the following performance metrics from cross-validation:

-   Confusion matrix\
-   Accuracy\
-   ROC curve and AUC

How well does the Random Forest model separate non-Severe from Severe patients based on proteomic profiles?

```{r exercise1-setup, message=FALSE, warning=FALSE, echo=FALSE}

# ── Packages ───────────────────────────────────────────────────────────────────
packages <- c(
  "readxl", "tidyverse", "janitor", "httr", "mice",
  "tidymodels", "ranger", "vip", "pROC", "kableExtra"
)
installed <- packages %in% rownames(installed.packages())
if (any(!installed)) install.packages(packages[!installed], dependencies = TRUE)
libs <- lapply(packages, library, character.only = TRUE)
```

```{r exercise1-data}

# ── Download Table 3 (Training Proteomics) ─────────────────────────────────────
mmc3_url  <- "https://ars.els-cdn.com/content/image/1-s2.0-S0092867420306279-mmc3.xlsx"
mmc3_file <- tempfile(fileext = ".xlsx")
invisible(GET(mmc3_url, write_disk(mmc3_file, overwrite = TRUE)))
df_mmc3_raw <- read_excel(mmc3_file, sheet = 2, skip = 1, na = c("", "NA", "/"))

# ── Download Metadata (Labels) ─────────────────────────────────────────────────
meta_url  <- "https://ars.els-cdn.com/content/image/1-s2.0-S0092867420306279-mmc1.xlsx"
meta_file <- tempfile(fileext = ".xlsx")
invisible(GET(meta_url, write_disk(meta_file, overwrite = TRUE)))
df_metadata_raw <- read_excel(meta_file, sheet = 2, na = c("", "NA", "/"))

# ── Clean Table 3 & Transpose ──────────────────────────────────────────────────
df_mmc3 <- df_mmc3_raw %>%
  clean_names() %>%
  rename(protein = 1) %>%
  select(-2)

df_mmc3_transposed <- df_mmc3 %>%
  column_to_rownames(var = "protein") %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column(var = "Patient_ID") %>%
  mutate(Patient_ID = toupper(Patient_ID))

# FIX: restore uppercase UniProt IDs
colnames(df_mmc3_transposed) <- toupper(colnames(df_mmc3_transposed))


# ── Clean Metadata ─────────────────────────────────────────────────────────────
df_labels <-suppressWarnings( df_metadata_raw %>%
  clean_names() %>%
  mutate(
    group = case_when(
      group_d == 0 ~ "Non-COVID-19",
      group_d == 1 ~ "Healthy",
      group_d == 2 ~ "Non-severe",
      group_d == 3 ~ "Severe",
      TRUE         ~ NA_character_
    ),
    group      = factor(group, levels = c("Healthy", "Non-COVID-19", "Non-severe", "Severe")),
    patient_id_a = toupper(str_trim(patient_id_a))
  ) %>%
  select(patient_id_a, group))

# ── Merge & Filter for Binary Classification ───────────────────────────────────
ml_data <- df_mmc3_transposed %>%
  inner_join(df_labels, by = c("PATIENT_ID" = "patient_id_a")) %>%
  filter(group %in% c("Severe", "Non-severe")) %>%
  mutate(group = droplevels(group)) %>%
  select(-PATIENT_ID)

# ── KEEP ONLY PROTEINS ─────────────────────────────

ml_data_protein <- ml_data %>%
  dplyr::select(
    group,
    dplyr::matches("^[OPQ][0-9A-Z]{5}$")
  )

cat("Group distribution in training set:\n")
print(table(ml_data_protein$group))


#--Split the dataset─────────────────────────────────
set.seed(123)

# 80/20 Split
data_split <- initial_split(ml_data_protein, prop = 0.8, strata = group)
train_data <- training(data_split)
test_data  <- testing(data_split)

cat("Training set:\n"); print(table(train_data$group))

cat("Test set:\n");     print(table(test_data$group))
# ── MICE Imputation (nur auf Trainingsdaten) ───────────────────────────────────
keep_cols <- train_data %>%
  select(-group) %>%
  select(where(~ mean(is.na(.)) < 0.8)) %>%
  colnames()


train_data <- train_data %>% select(group, all_of(keep_cols))
test_data  <- test_data  %>% select(group, all_of(keep_cols))

train_medians <- sapply(select(train_data, -group), median, na.rm = TRUE)

train_data <- train_data %>%
  mutate(across(-group, ~ ifelse(is.na(.), train_medians[cur_column()], .)))
test_data <- test_data %>%
  mutate(across(-group, ~ ifelse(is.na(.), train_medians[cur_column()], .)))

# ── Check ──────────────────────────────────────────────────────────────────────
#cat("Missing in train_data:", sum(is.na(train_data)), "\n")
#cat("Missing in test_data:",  sum(is.na(test_data)),  "\n")


# CV nur auf dem Trainingsset
cv_folds <- vfold_cv(train_data, v = 5, strata = group)

```

```{r exercise1-model, fig.width=10, fig.height=5}

set.seed(123)

rf_recipe <- recipe(group ~ ., data = train_data) %>%
  step_filter_missing(all_predictors(), threshold = 0.4) %>%
  step_impute_knn(all_numeric_predictors(), neighbors = 5) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

rf_spec_tune <- rand_forest(trees = 500, mtry = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_workflow_tune <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_spec_tune)

rf_param  <- extract_parameter_set_dials(rf_workflow_tune) %>% finalize(train_data)

mtry_grid <- grid_regular(rf_param, levels = 5)

rf_tune_results <- tune_grid(
  rf_workflow_tune,
  resamples = cv_folds,
  grid      = mtry_grid,
  metrics   = metric_set(accuracy, roc_auc, sensitivity, specificity)
)

autoplot(rf_tune_results) +
  theme_minimal() +
  labs(title    = "Tuning Results for Random Forest",
       subtitle = "Performance metrics across different mtry values")
```
The Random Forest achieved high discriminative ability but very low sensitivit, indicating poor detection of Severe cases and likely overfitting due to the small sample size.

```{r exercise1-eval, fig.width=8, fig.height=5}

best_mtry      <- select_best(rf_tune_results, metric = "roc_auc")
final_workflow <- finalize_workflow(rf_workflow_tune, best_mtry)

final_res <- fit_resamples(
  final_workflow,
  resamples = cv_folds,
  metrics   = metric_set(accuracy, roc_auc, sensitivity, specificity),
  control   = control_resamples(save_pred = TRUE)
)

# ── Performance metrics table ──────────────────────────────────────────────────
collect_metrics(final_res) %>%
  select(.metric, mean, std_err) %>%
  kable(caption = "Cross-validation performance metrics", digits = 3) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE, position = "center")
```
Cross-validation shows high AUC with perfect sensitivity but very low specificity, indicating poor discrimination.
```{r}
# ── Confusion matrix ───────────────────────────────────────────────────────────
preds <- collect_predictions(final_res)
print(conf_mat(preds, truth = group, estimate = .pred_class))
```

The confusion matrix reveals that 9 of 10 Severe cases are misclassified as Non-severe, demonstrating very low sensitivity and strong bias toward the majority class despite high overall ranking performance.

```{r}
# ── Sensitivity & Specificity ──────────────────────────────────────────────────
preds <- preds %>%
  mutate(group = factor(group, levels = c("Non-severe", "Severe")))

sens_res <- yardstick::sensitivity(preds, truth = group, estimate = .pred_class,
                                   event_level = "second")
spec_res <- yardstick::specificity(preds, truth = group, estimate = .pred_class,
                                   event_level = "second")

cat(sprintf("Sensitivity: %.3f\nSpecificity: %.3f\n",
            sens_res$.estimate, spec_res$.estimate))
```
The model detects only 10% of Severe cases (sensitivity = 0.10) while correctly classifying all Non-severe patients (specificity = 1.00), indicating poor clinical usefulness due to failure to identify Severe patients.

```{r}
# ── ROC curve ─────────────────────────────────────────────────────────────────
roc_curve(preds, truth = group, .pred_Severe) %>%
  autoplot() +
  ggtitle("ROC Curve: Training Cohort (5-fold CV)") +
  theme_minimal()
```
The ROC curve shows excellent ranking ability (AUC = 0.97), but the low sensitivity indicates poor classification of Severe patients at the selected decision threshold.

```{r}

# ── Evaluation auf internem Test-Set (20 %) ────────────────────────────────────
final_fit <- fit(final_workflow, data = train_data)

test_preds <- predict(final_fit, new_data = test_data) %>%
  bind_cols(predict(final_fit, new_data = test_data, type = "prob")) %>%
  bind_cols(test_data %>% select(group)) %>%
  mutate(group = factor(group, levels = c("Non-severe", "Severe")))

cat("── Internal Test Set (20 %) ──\n")
print(conf_mat(test_preds, truth = group, estimate = .pred_class))

acc_test  <- accuracy(test_preds, truth = group, estimate = .pred_class)
sens_test <- yardstick::sensitivity(test_preds, truth = group,
                                    estimate = .pred_class, event_level = "second")
spec_test <- yardstick::specificity(test_preds, truth = group,
                                    estimate = .pred_class, event_level = "second")

cat(sprintf("Accuracy:    %.3f\nSensitivity: %.3f\nSpecificity: %.3f\n",
            acc_test$.estimate, sens_test$.estimate, spec_test$.estimate))

roc_curve(test_preds, truth = group, .pred_Severe) %>%
  autoplot() +
  ggtitle("ROC Curve: Internal Test Set (20 %)") +
  theme_minimal()


```
The 5-fold cross-validation (n = 24) shows excellent ranking ability (AUC = 0.97) but very low sensitivity (0.10), indicating that most Severe cases are missed at the chosen threshold. On the independent test set (n = 7), the model achieved 71% accuracy, with sensitivity of 0.33 and specificity of 1.00, correctly identifying only 1 of 3 Severe patients. These results suggest limited reliability and instability due to the small sample size.

---

# Exercise 2

**Key finding:** Several top Random Forest predictors, including CRP, SAA1, CFB, and ITIH3, match published COVID-19 severity biomarkers, confirming the biological validity of the model.



**Identify and interpret the most important features**

Using the trained Random Forest model from Exercise 1, identify the 25 most important protein features contributing to the classification. Discuss their biological relevance.

Compare your selected features to the proteins reported by the authors in Supplementary Table 5.

```{r exercise2-importance, fig.width=8, fig.height=6}

# ── Fit final model on full dataset ────────────────────────────────────────────
final_fit_full <- fit(final_workflow, data = ml_data_protein)
fit_model      <- extract_fit_parsnip(final_fit_full)

# ── Top 25 features ────────────────────────────────────────────────────────────
top_25 <- vip::vi(fit_model) %>%
  slice_max(Importance, n = 25)

top_25 %>%
  kable(caption = "Top 25 most important proteins (Random Forest)", digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE, position = "center")
```
The table lists the 25 proteins with highest predictive importance in the Random Forest model.
```{r exercise2-validation, fig.width=8, fig.height=6}

# ── Download Table S5 ─────────────────────────────────────────────────────────

url_mmc5  <- "https://ars.els-cdn.com/content/image/1-s2.0-S0092867420306279-mmc5.xlsx"
mmc5_file <- tempfile(fileext = ".xlsx")

invisible(GET(url_mmc5, write_disk(mmc5_file, overwrite = TRUE)))

df_validation <- read_excel(mmc5_file, sheet = 2)

validation_clean <- df_validation %>%
  rename(
    protein_id = `Proteins/Metabolites`,
    gene_symbol = `Gene Symbol`
  ) %>%
  filter(grepl("^[OPQ][0-9A-Z]{5}$", protein_id))



# ── Compare Top 25 to paper ────────────────────────────────────────────────────
comparison_table <- top_25 %>%
  inner_join(validation_clean, by = c("Variable" = "protein_id")) %>%
  select(Variable, gene_symbol, Importance) %>%
  arrange(desc(Importance))

comparison_table %>%
  kable(caption = "Top 25 proteins vs. published DE proteins (Table S5)", digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE, position = "center")

matched <- sum(top_25$Variable %in% validation_clean$protein_id)
cat(sprintf("Top 25 features matched in Table S5: %d / 25\n", matched))

# ── Validation plot ────────────────────────────────────────────────────────────
ggplot(comparison_table,
       aes(x = reorder(gene_symbol, Importance),
           y = Importance)) +
  geom_col(fill = "steelblue") +
  coord_flip() +
  labs(title = "Top 25 important proteins",
       x = "Protein",
       y = "Importance") +
  theme_minimal()



```
Most of the top Random Forest predictors show consistent regulation patterns with published proteomic results, confirming the biological relevance of the model.

Several of the most important proteins identified by the Random Forest model, including CRP, SAA1, CFB, and ITIH3, are well-known markers of inflammation and immune activation. CRP and SAA1 are acute-phase proteins strongly associated with severe COVID-19, while complement proteins such as CFB reflect immune system dysregulation. The overlap with published findings confirms that the model captures biologically meaningful severity-related signals.

---

# Exercise 3


**Key finding:** The model achieved good external accuracy but reduced sensitivity, indicating limited generalizability.


**External prediction on an independent test cohort**

Using the final Random Forest model trained on the full training cohort, predict disease severity for patients in the independent test cohort provided in Supplementary Table 4.

Your task is to:

-   Apply the trained model to the test cohort.\
-   Obtain predicted probabilities of Severe disease for each patient.\
-   Assign predicted class labels based on these probabilities.\
-   Compare predictions to the true clinical labels.

How does the model perform on the independent test cohort?

```{r exercise3-data}

# ── Download Table S4 (Test Cohort) ───────────────────────────────────────────
url_mmc4  <- "https://ars.els-cdn.com/content/image/1-s2.0-S0092867420306279-mmc4.xlsx"
mmc4_file <- tempfile(fileext = ".xlsx")
invisible(GET(url_mmc4, write_disk(mmc4_file, overwrite = TRUE)))

df_mmc4_raw <- read_excel(mmc4_file, sheet = 2, skip = 1)

external_test_data <- df_mmc4_raw %>%
  clean_names() %>%
  select(-2) %>%
  rename(protein_id = 1) %>%
  column_to_rownames("protein_id") %>%
  t() %>%
  as.data.frame() %>%
  rownames_to_column("Patient_ID") %>%
  mutate(Patient_ID = toupper(Patient_ID))

external_test_data <- external_test_data %>%
  inner_join(df_labels, by = c("Patient_ID" = "patient_id_a")) %>%
  filter(group %in% c("Severe", "Non-severe")) %>%
  mutate(group = droplevels(group))

cat("External test cohort — group distribution:\n")
print(table(external_test_data$group))
```

```{r exercise3-retrain}

# ── Feature intersection ───────────────────────────────────────────────────────
train_features  <- colnames(ml_data_protein)
test_features   <- colnames(external_test_data)
common_proteins <- intersect(setdiff(train_features, "group"),
                             setdiff(test_features, "Patient_ID"))

cat(sprintf("Common proteins between training and test cohort: %d\n",
            length(common_proteins)))

missing_in_test <- setdiff(top_25$Variable, common_proteins)
cat(sprintf("Top 25 features missing in test cohort: %d\n", length(missing_in_test)))

ml_data_robust       <- ml_data_protein %>% select(group, all_of(common_proteins))
external_test_robust <- external_test_data %>%
  select(group, Patient_ID, all_of(common_proteins))

# ── Retrain on full training set (common proteins only) ────────────────────────
final_recipe_robust <- recipe(group ~ ., data = ml_data_robust) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_zv(all_predictors())

final_spec_robust <- rand_forest(trees = 1000, mtry = best_mtry$mtry) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

final_workflow_robust <- workflow() %>%
  add_recipe(final_recipe_robust) %>%
  add_model(final_spec_robust)

final_fit_robust <- fit(final_workflow_robust, data = ml_data_robust)
```

```{r exercise3-eval, fig.width=7, fig.height=5}

# ── Predict on external cohort ─────────────────────────────────────────────────
predictions <- predict(final_fit_robust, new_data = external_test_robust) %>%
  bind_cols(predict(final_fit_robust, new_data = external_test_robust,
                    type = "prob")) %>%
  bind_cols(external_test_robust %>% select(group, Patient_ID)) %>%
  mutate(group = factor(group, levels = c("Non-severe", "Severe")))

# ── Confusion matrix ───────────────────────────────────────────────────────────
print(conf_mat(predictions, truth = group, estimate = .pred_class))

# ── Accuracy ───────────────────────────────────────────────────────────────────
acc_res <- accuracy(predictions, truth = group, estimate = .pred_class)
cat(sprintf("External validation accuracy: %.3f\n", acc_res$.estimate))

# ── Sensitivity & Specificity ──────────────────────────────────────────────────
sens_ext <- yardstick::sensitivity(predictions, truth = group, estimate = .pred_class,
                                   event_level = "second")
spec_ext <- yardstick::specificity(predictions, truth = group, estimate = .pred_class,
                                   event_level = "second")

cat(sprintf("External Sensitivity (Severe detection): %.3f\n", sens_ext$.estimate))
cat(sprintf("External Specificity (Non-severe detection): %.3f\n", spec_ext$.estimate))

# ── AUC & ROC curve ────────────────────────────────────────────────────────────
predictions <- predictions %>%
  mutate(group_binary = as.numeric(group) - 1)

roc_obj <- roc(response  = predictions$group_binary,
               predictor = predictions$.pred_Severe,
               direction = "<",
               ci        = TRUE)

auc_value <- as.numeric(roc_obj$auc)
auc_ci    <- as.numeric(roc_obj$ci)

cat(sprintf("External validation AUC: %.3f  (95%% CI: [%.3f, %.3f])\n",
            auc_value, auc_ci[1], auc_ci[3]))

plot(roc_obj,
     main = "ROC Curve: External Validation (Table S4)",
     xlab = "False Positive Rate (1 – Specificity)",
     ylab = "True Positive Rate (Sensitivity)",
     col  = "darkblue",
     lwd  = 2.5)
legend("bottomright",
       paste0("AUC = ", round(auc_value, 3),
              "\n95% CI: [", round(auc_ci[1], 3), ", ", round(auc_ci[3], 3), "]"),
       bty = "n", cex = 1)
grid(col = "lightgray", lty = "dotted")
```

The model achieved good external accuracy but low sensitivity, indicating limited ability to detect Severe patients and reduced generalizability.


# Conclusion

Random Forest successfully identified biologically relevant proteomic predictors of COVID-19 severity and demonstrated reasonable predictive performance.

---

# session info {.unnumbered}

```{r session-info, results='asis', echo=FALSE, message=FALSE}
sessionInfo()
```
